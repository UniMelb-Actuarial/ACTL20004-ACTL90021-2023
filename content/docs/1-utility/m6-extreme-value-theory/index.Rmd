---
# bookHidden: true
# bookSearchExclude: true
weight: 60
title: "M6 Extreme Value Theory"
subtitle: "General Insurance Modelling : Actuarial Modelling III [^1]"
author: "Professor Benjamin Avanzi"
institute:  |
  ![](../../../../static/img/PRIMARY_A_Vertical_Housed_RGB.png){width=1.2in}  
date: '`r format(Sys.time(), "%d %B %Y")`'
output:
  beamer_presentation:
    toc: true
    number_sections: true
    df_print: kable
    slide_level: 3
    theme: "CambridgeUS"  
    colortheme: "dolphin"  
    fonttheme: "default"
bibliography: ../../../../static/libraries.bib
classoption: t,handout 
header-includes:
  - \graphicspath{{../../../static/}}
  - \usepackage{color}
  - \usepackage{hyperref}
  - \usepackage{marvosym}
  - \usepackage{amsmath}
  - \usepackage{amsthm}
  - \usepackage{amsfonts}
  - \usepackage{array}
  - \usepackage{booktabs}
  - \usepackage{verbatim}
  - \usepackage[english]{varioref}
  - \usepackage{natbib}
  - \usepackage{actuarialangle}
  - \usepackage{pgfpages}   
  - \pgfdeclareimage[height=1cm]{university-logo}{../../../../static/img/PRIMARY_A_Vertical_Housed_RGB.png}
  - \pgfdeclareimage[height=2.5cm]{university-logo2}{../../../../static/img/PRIMARY_A_Vertical_Housed_RGB.png}
  - \logo{\raisebox{-3ex}[0pt]{\pgfuseimage{university-logo}}}
  - \AtBeginSection[]{     \begin{frame}    \tableofcontents[sectionstyle=show/shaded,subsectionstyle=hide/hide/hide]     \end{frame}  \addtocounter{framenumber}{-1}}
  - \AtBeginSubsection[]{     \begin{frame}    \tableofcontents[sectionstyle=show/hide,subsectionstyle=show/shaded/hide]      \end{frame}  \addtocounter{framenumber}{-1}} # to remove this you need to also change "slide_level" to 2
  - \definecolor{DolphinBlue}{RGB}{51,44,159}
  - \setbeamerfont{section in toc}{size=\normalsize}
  - \setbeamerfont{subsection in toc}{size=\normalsize}
  - \pretocmd{\tableofcontents}{\setlength{\parskip}{.2em}}{}{}
  - \setbeamertemplate{footline}{\hspace*{.4em} \raisebox{1.5ex}[0pt]{\textcolor{DolphinBlue}{\insertframenumber/\inserttotalframenumber}}}
  #- \setbeamertemplate{footline}{\hspace*{.4em} \raisebox{1.5ex}[0pt]{\textcolor{DolphinBlue}{\insertframenumber}}}
  #- \apptocmd{\tableofcontents}{\linespread{1.0}}{}{}
  # - \setbeamerfont{subsubsection in toc}{size=fontsize}
  - \newcommand{\adv}{$\maltese$}
---


```{r setup, include=FALSE}
library(knitr)

### Change these two variables to change name of image folder and output file
#
graphics_folder <- "module-6"
output_name <- "23-GIM-M6-lec" # change latest slides below, too!

### Do not change the following unless you know what you are doing
# output width
options(width = 70)
source(sprintf("%s/static/slide-setup.R", rprojroot::find_root(".Rprofile")))
graphics_path <-  sprintf("/img/%s", graphics_folder)    # Directory of where files are kept for this page

if (exists("output_type") && output_type == "beamer") {
  graphics_path <-  sprintf("%s/static/img/%s", root_dir, graphics_folder)    # Directory of where files are kept for this page
}

# Cache chunk outputs
opts_chunk$set(cache=TRUE)
```
 
```{r, eval=F, include=F}
# Run this chunk to export to pdf
# Make sure that the setup chunk is run first
#  (to set graphics_folder and output_name)
#
#  The file is also saved before running this file
output_type <- "beamer"

# install.packages("rprojroot")
root_dir <- rprojroot::find_root(".Rprofile")                               # Directory of Project
output_dir <- sprintf("%s/static/output", root_dir)                         # Directory of output pdfs
curr_file <- sprintf("%s/index.Rmd", getwd())
### Use this and run the lines from graphics_folder to render() to create pdf
rmarkdown::render(input = curr_file, output_file = output_name, output_dir = output_dir)
# rmarkdown::render(input = curr_file, output_file = output_name, output_dir = "/Users/bavanzi/Dropbox/My Mac (3160L-201016-M)/Documents/GitHub/ACTL30007-ACTL90020-2022/static/output")

### Other notes to self
#The first 4 lines.
#```{r test, output.lines=4}
#```
#Remove the first 8 lines.
#```{r test, output.lines=-(1:8)}
#```
#From 8 to 15.
#```{r test, output.lines=8:15}
#```

# install.packages("formatR")
# collapse=TRUE #for collapsing output with code
```
 
```{r, include=FALSE}
library(astsa)
library(xts)
library(readxl)
library(writexl)
library(stats)
library(actuar)
library(fitdistrplus)
library(tidyverse)
library(evir)
library(extRemes)
```
 
[^1]: References: Unit 4 of CS2 \linebreak
$\text{     }\text{    }\text{ }\text{ }$ and @GiKa16 | $\; \rightarrow$ [\textcolor{blue}{\underline{latest slides}}](https://gim-am3.netlify.app/output/23-GIM-M6-lec.pdf) 


# Introduction

## Extreme events 

- financial (and claims) data has often leptokurtic severity (more peaked - heavy tailed - than normal)
- reasons include heteroscedasticity (stochastic volatility)
- almost by definition, extreme events happen with low frequency
- but extreme events are the ones leading to financial distress, possibly ruin
- accurate fitting is crucial for capital and risk management (risk based capital, reinsurance, ART such as securitisation via CAT bonds)
- fitting presents specific issues:
    - there is little data to work from
    - when fitting a whole distribution, the big bulk of the data (around the mode) overpowers observations in the tails in the optimisation routine (MLE), leading to poor fit in the tail
    - sometimes (in fact, almost always) a good distribution for the far tail is different from that for the more common outcomes.

## Framework

- Consider iid random variables $X_i$, $i=1,\ldots,n$, with df $F$
- We denote the order statistics
$$X_{n,n} \le X_{n-1,n} \le \ldots \le X_{1,n}$$
such that
$$X_{n,n}=\min(X_1,\ldots,X_n) \text{ and }X_{1,n}=\max(X_1,\ldots,X_n).$$
- Specifically, we may be interested in
$$\begin{array}{rcl}
\text{average of }k\text{ largest losses} &=& \displaystyle \sum_{r=1}^k\frac{X_{r,n}}{k} \\
\text{empirical mean excess function above }u &=& \displaystyle \sum_{r=1}^{n_u}\frac{(X_{r,n}-u)}{n_u},\text{where} \\
n_u = \# \left\{ 1\le r\le n:X_r>u\right\}&
\end{array}$$

<!-- $n_u = \# \left\{ 1\le r\le n:X_r>u\right\}$ -->

## Motivating example [@EmReSa99]
Consider the following situation:

- Let $X_i$ be exponential mean $1/\lambda=10$
- We have $n=100$ observations
- The maximum observed was 50. How likely is it if the model is correct? What if the maximum had been 100?

The probability that the maximum $X_{1,n}$ is at least $x$ is
$$\Pr[X_{1,n}>x] =1-\left( \Pr[X\le x]\right)^{100} = 1-\left( 1-e^{-x/10}\right)^{100},$$
and hence
$$\begin{array}{rcl}
\Pr[X_{1,n}>50] &=& 0.4919, \\
\Pr[X_{1,n}>100] &=& 0.00453.
\end{array}$$

## A limiting argument
We now perform the following calculations:
$$\Pr\left[ \frac{X_{1,n}}{\alert{10}}-\alert{\log n}\le x\right] = \Pr [X_{1,n} \le 10(x+\log n)] = \left(1-\frac{e^{-x}}{n}\right)^n,$$
where the elements in red were chosen based on an educated guess, so that
$$\lim_{n\rightarrow\infty} \Pr\left[\frac{X_{1,n}}{\alert{10}}-\alert{\log n} \le x\right]=\lim_{n\rightarrow\infty} \left(1-\frac{e^{-x}}{n}\right)^n=e^{-e^{-x}} \equiv \Lambda(x),$$
which is the df of a Gumbel (or double exponential) random variable.




### General result for $X$ exponential $(\lambda)$

We can easily generalise the previous result to any $\lambda$ and $n$. Noting that 10 was actually $1/\lambda$ before, we had in summary
$$\Pr [X_{1,n} \le (x+\log n)/\lambda] \approx \Lambda(x).$$
If we set 
$$ y = (x+\log n)/\lambda$$ 
then we get
$$\Pr[X_{1,n}\le y] \approx \Lambda \left( \lambda y -\log n\right).$$
In the limit $(n \rightarrow \infty)$, the approximation is exact.


This means that for large $n$, one can approximate the distribution of the maximum of a set of iid exponentially distributed $X_i$'s (with parameter $\lambda$) with a Gumbel distribution.


---

Indeed, this works well for our example, especially in the tail:
$$\begin{array}{rcl}
\Pr[X_{1,n}>50] &\approx& 1-e^{-e^{-(50/10-\log 100)}}=0.4902, \\
\Pr[X_{1,n}>100] &\approx& 0.00453.
\end{array}$$


### Questions

- This is nice, but not all random variables are exponential.
- Can we generalise such a limiting approach to any df?

**Yes!**

- Can we find norming constants (such as $\lambda$ and $\log n$ in the exponential case) in general?  
That is, how do we find $a_n$ and $b_n$ so that
$$\lim_{n\rightarrow \infty} \Pr \left[ \frac{X_{1,n}-b_n}{a_n} \le x\right]$$
exists?

**We can!** (there are only three cases to consider), but we won't discuss that here [see, e.g., @EmKlMi97  
for the proofs]




# Generalised Extreme Value distribution

## The GEV distribution

### Parallel with CLT


`r colFmt("The theoretical results of the previous section lead to an extreme value complement or homologue, for the tails, of the CLT approximation (which works only around the middle of the distribution).",'red')`

- In the exponential example, the "normalised" maximum 
$$ \frac{X_{1,n}}{\text{mean of X}} - \log n$$
for infinite sample $n$ stabilised to a Gumbel distribution $\Lambda$.
- This generalises to any distribution for $X$ - the distribution of the normalised maximum then becomes a "Generalised Extreme Value" distribution, a special case of which is the Gumbel distribution.

### Extreme Value Distributions

The Gumbel distribution $\Lambda(x)$ derived earlier is a special case of a general distribution family
$$H_{\gamma;\mu,\sigma}(x)=\exp \left\{ - \left(1+\gamma\frac{x-\mu}{\sigma}\right)_+^{-\frac{1}{\gamma}}\right\},\;\gamma \in \mathbb{R}, \mu\in\mathbb{R}, \sigma>0.$$
This Generalised Extreme Value distribution (GEV) encompasses **all** extreme value distributions, of which we distinguish three cases:

- $\gamma<0$: upper bounded Weibull (Sometimes referred to as the reverse or reflected Weibull) df with $x<\mu-\sigma/\gamma$
- $\gamma=0$: Gumbel df with $x\in \mathbb{R}$
- $\gamma>0$: Fréchet-type df with $x>\mu- \sigma/\gamma$

The last one relates to the heavy tailed distributions (typically with moments existing only up to a certain order, infinite afterwards),  
and is the most relevant to actuarial applications.


### Three cases

```{r fig.align='center',out.width="100%", echo=F}
include_graphics(sprintf("%s/EVMtable.png", graphics_path), error = F)
```

[Unit 4, Page 4, @IoA20] Note $\alpha \equiv \mu$ and $\beta \equiv \sigma$


---

```{r 3cases,fig.show="hide"}
GenEV<-function(x,alpha,beta,gamma) {
  1/beta*(1+gamma*(x-alpha)/beta)^(-(1+1/gamma))*exp(-((1+gamma*(x-alpha)/beta)^(-1/gamma)))}

par(mfrow=c(1,3),oma=c(0,0,3,0))
plot(-4000:2000/1000,GenEV(-4000:2000/1000,0,1,-.5),main="Density with gamma=-0.5 (Weibull)",xlim=c(-4,4),xlab="",ylab="",cex=1)
plot(-4000:4000/1000,GenEV(-4000:4000/1000,0,1,0.00001),main="Density of GEV with gamma=0 (Gumbel)",xlim=c(-4,4),xlab="",ylab="",cex=1)
plot(-2000:4000/1000,GenEV(-2000:4000/1000,0,1,.5),main="Density of GEV with gamma=0.5 (Fréchet)",xlim=c(-4,4),xlab="",ylab="",cex=1)
mtext("Density of GEV with mu=0, sigma=1, and various values of gamma",outer = TRUE,cex = 1.5)
```

---

```{r 3cases,echo=FALSE,fig.height=7}
```

### Estimation: Block Maxima

- The EVD is a distribution of maxima
- To fit it we need a sample of maxima
- To achieve this, the data is transformed into block maxima
    1. assume we have $n$ data points
    \emph{(say, monthly data over 1000 years, so $n=12000$)}
    2. consider blocks of length $m$
    \emph{(say, all observations in a given year such that $m=12$)}
    3. the block maxima are the equal to the maximum observation within a block
    \emph{(the maximum value within each of the 1000 years)}
    4. we have then a sample size of $n/m$
    \emph{(our sample size of maxima is of size 12000/12=1000)}
- The block maxima are then fitted to the EVD
- The larger the block size, the closer to the asymptotic distribution we get, but the less data we have: conflicting requirements
- In R, this can be done efficiently with the function `fevd` of the package `extRemes` [@GiKa16]

## Case study - data

### Data sets

We consider three simulated data sets with $n=12000$ (provided along with the lecture notes), expected value \$ $10,\!000$ and variance \$ $9,\!000^2$:

|month | beta | gamm | logg |
|:----:|-----:|----:|-----:|
| $\vdots$ |  |  | |
| 61 | 12862.84 | 33493.02 | 18773.47 |
| 62 | 408.64 | 6070.20 | 7564.21 |
| 63 | 2404.87 | 26093.36 | 5083.10 |
| 64 | 4753.73 | 3773.61 | 2070.88 |
| 65 | 594.57 | 3885.25 | 43601.62 |
| 66 | 16805.64 | 6881.97 | 6163.90 |
| 67 | 4335.52 | 5163.38 | 8158.35 |
| 68 | 5575.15 | 24401.76 | 19172.13 |
| $\vdots$ |  |  | |

---

```{r casestudy}
claims <-as_tibble(read_excel("simulated-claims.xlsx"))

summary(claims$beta)
summary(claims$gamm)
summary(claims$logg)
```

---

```{r casestudy2,fig.height=6}
plotdist(claims$beta)
```

---

```{r casestudy3,fig.height=6}
plotdist(claims$gamm)
```

---

```{r casestudy4,fig.height=6}
plotdist(claims$logg)
```



### Block maxima

Creating block maxima

```{r blockmaxima}
#block maxima index
claims$block <- (claims$month-1) %/% 12+1
# %/% gives the integer part of the result of the division
blockmax <- tibble(betablock=aggregate(beta~block,claims,max)$beta,
                   gammblock=aggregate(gamm~block,claims,max)$gamm,
                   loggblock=aggregate(logg~block,claims,max)$logg)
# have a look at what the aggregate function does
```

We now have a sample of $12000/12=1000$ block maxima.

What do these look like in comparison with the original data?

---

```{r blockmaxima2}
claims <-as_tibble(read_excel("simulated-claims.xlsx"))

summary(blockmax$betablock)
summary(blockmax$gammblock)
summary(blockmax$loggblock)
```

---

```{r blockmaxima3,fig.height=6}
plotdist(blockmax$betablock)
```

---

```{r blockmaxima4,fig.height=6}
plotdist(blockmax$gammblock)
```

---

```{r blockmaxima5,fig.height=6}
plotdist(blockmax$loggblock)
```

---

```{r blockmaxima6,fig.height=5}
par(mfrow=c(1,2))
plot(density(claims$beta),main="Density of the beta claims",xlab="Claim amounts ($)",xlim=c(0,max(claims$beta)))
plot(density(blockmax$betablock),main="Density of the beta block maxima",xlab="Maximums over consecutive periods of 12 months ($)",xlim=c(0,max(claims$beta)))
```


---

```{r blockmaxima7,fig.height=5}
par(mfrow=c(1,2))
plot(density(claims$gamm),main="Density of the gammormal claims",xlab="Claim amounts ($)",xlim=c(0,max(claims$gamm)))
plot(density(blockmax$gammblock),main="Density of the gammormal block maxima",xlab="Maximums over consecutive periods of 12 months ($)",xlim=c(0,max(claims$gamm)))

```



---

```{r blockmaxima8,fig.height=5}
par(mfrow=c(1,2))
plot(density(claims$logg),main="Density of the log-gamma claims",xlab="Claim amounts ($)",xlim=c(0,max(claims$logg)))
plot(density(blockmax$loggblock),main="Density of the log-gamma block maxima",xlab="Maximums over consecutive periods of 12 months ($)",xlim=c(0,max(claims$logg)))
```

<!-- --- -->

<!-- ```{r blockmaxima9,fig.height=5} -->
<!-- par(mfrow=c(1,2)) -->
<!-- plot(density(log(claims$logg)),main="Density of the log-gamma claims",xlab="Claim amounts ($)",xlim=c(0,max(log(claims$logg)))) -->
<!-- plot(density(log(blockmax$loggblock)),main="Density of the log-gamma block maxima",xlab="Maximums over consecutive periods of 12 months ($)",xlim=c(0,max(log(claims$logg)))) -->
<!-- ``` -->

---

```{r blockmaxima10,fig.height=5}
par(mfrow=c(1,3))
plot(density(blockmax$betablock),main="Density of the beta block maxima",xlab="Maximums over consecutive periods of 12 months ($)")
plot(density(blockmax$gammblock),main="Density of the gammormal block maxima",xlab="Maximums over consecutive periods of 12 months ($)")
plot(density(blockmax$loggblock),main="Density of the loggamma block maxima",xlab="Maximums over consecutive periods of 12 months ($)")
```

## Case study - fitting

### Fitting results

For example for the `logg` BM we use the following code:

- `fit.logg <- fevd(blockmax$loggblock)` will perform the fit
- `fit.logg` will display the results of the fitting
- `plot(fit.logg)` will plot goodness of fit graphical analysis
- `plot(fit.logg,"trace")` will plot the likelihood function around the chosen parameters

More details can be found in @GiKa16.


---

Results (MLE with standard errors in parentheses) are

| | location $\mu$ | scale $\sigma$ | shape $\gamma$ |
|-|-------------|-------------|------------|
| beta | 25644.93 (163.26) |  4695.76 (130.07) | -0.6012 (0.0212) |
| gamma | 24069.68 (288.51) | 8225.98 (215.80) | -0.0243 (0.0222) |
| loggamma | 21466.22 (305.26) | 8496.86 (251.97) | 0.2299 (0.0278) |

Note:

- These correspond to the three distinct cases introduced before as expected (look at the sign of $\gamma$)
- Whether $\gamma\neq 0$ for gamma can be formally tested in different ways [see @GiKa16], but the standard error for $\gamma^{\text{gamm}}$ suggests a null hypothesis of $\gamma^{\text{gamm}} = 0$ would not be rejected.


---

```{r block-fitbeta1,warning=FALSE,collapse=TRUE,output.lines=10:26}
fit.beta <- fevd(blockmax$betablock)
fit.beta
```

---

```{r block-fitbeta2,warning=FALSE,fig.height=6.5}
plot(fit.beta)
```

---

```{r block-fitbeta3,warning=FALSE,fig.height=6}
plot(fit.beta,"trace")
```

---

```{r block-fitgamm1,warning=FALSE,collapse=TRUE,output.lines=10:26}
fit.gamm <- fevd(blockmax$gammblock)
fit.gamm
```

---

```{r block-fitgamm2,warning=FALSE,fig.height=6.5}
plot(fit.gamm)
```

---

```{r block-fitgamm3,warning=FALSE,fig.height=6}
plot(fit.gamm,"trace")
```

---

```{r block-fitlogg1,warning=FALSE,collapse=TRUE,output.lines=10:26}
fit.logg <- fevd(blockmax$loggblock)
fit.logg
```

---

```{r block-fitlogg2,warning=FALSE,fig.height=6.5}
plot(fit.logg)
```

---

```{r block-fitlogg3,warning=FALSE,fig.height=6}
plot(fit.logg,"trace")
```



## Case study - choice of block size

Remember that

-  the larger the block, the closer we are to the limiting distribution  
(this is good)

However

- the larger the block, the least data we have  
(this is bad)

Here we aim to illustrate the impact on the parameter estimates of increasing block sizes.

### Convergence with increasing block sizes

```{r blockfit,warning=FALSE,size="scriptsize"}
#block sizes
blocksizes <- c(1,2,3,4,6,12,18,24,30,60,c(1:20*120))

results <- c()

for (i in blocksizes) {
  #number of full blocks to work with
  numbers <- floor(length(claims$beta)/max(blocksizes))
  #trimming claims vector
  claims_trimmed <- claims[1:(numbers*i),]
  #block maxima
  claims_trimmed$block <- (claims_trimmed$month-1) %/% i+1
  blockmax2 <- tibble(betablock2=aggregate(beta~block,claims_trimmed,max)$beta,
                     gammblock2=aggregate(gamm~block,claims_trimmed,max)$gamm,
                     loggblock2=aggregate(logg~block,claims_trimmed,max)$logg)
  #fitting
  fit.beta2 <- fevd(blockmax2$betablock2)
  fit.gamm2 <- fevd(blockmax2$gammblock2)
  fit.logg2 <- fevd(blockmax2$loggblock2)
  results <- rbind(results,c(i,as.double(fit.beta2$results$par[1:3]),as.double(fit.gamm2$results$par[1:3]),as.double(fit.logg2$results$par[1:3])))
}

```

---

```{r blockfit1,fig.show="hide"}
par(mfrow=c(1,3))
plot(results[,1],results[,2],pch=20,main="Convergence of mu (betablock)",xlab="Block size")
plot(results[,1],results[,3],pch=20,main="Convergence of sigma (betablock)",xlab="Block size")
plot(results[,1],results[,4],pch=20,main="Convergence of gamma (betablock)",xlab="Block size")
```

```{r blockfit2,fig.show="hide"}
par(mfrow=c(1,3))
plot(results[,1],results[,5],pch=20,main="Convergence of mu (gammblock)",xlab="Block size")
plot(results[,1],results[,6],pch=20,main="Convergence of sigma (gammblock)",xlab="Block size")
plot(results[,1],results[,7],pch=20,main="Convergence of gamma (gammblock)",xlab="Block size")
```
```{r blockfit3,fig.show="hide"}
par(mfrow=c(1,3))
plot(results[,1],results[,8],pch=20,main="Convergence of mu (loggblock)",xlab="Block size")
plot(results[,1],results[,9],pch=20,main="Convergence of sigma (loggblock)",xlab="Block size")
plot(results[,1],results[,10],pch=20,main="Convergence of gamma (loggblock)",xlab="Block size")
```

---

```{r blockfit1,echo=FALSE,fig.height=7}
```

---

```{r blockfit2,echo=FALSE,fig.height=7}
```


---

```{r blockfit3,echo=FALSE,fig.height=7}
```



## Moments 

Let $Z$ follow a GEV( $\mu$ , $\sigma$ , $\gamma$ ). We have then
$$E[Z]=\left\{ \begin{array}{rc}
\mu+\sigma\left( \Gamma(1-\gamma)-1\right)/\gamma, &\gamma\neq 0, \gamma <1, \\
\mu+\sigma \cdot e,&\gamma =0, \\
\infty , &\gamma \ge 1.
\end{array}
\right.$$
$$Var(Z) = \left\{ \begin{array}{rc}
\sigma^2\left[ \Gamma(1-2\gamma)-\Gamma(1-\gamma)^2\right]/\gamma^2, &\gamma\neq 0, \gamma <1/2, \\
\sigma^2 \cdot\pi/6,&\gamma =0, \\
\infty , &\gamma \ge 1/2.
\end{array}
\right.$$

\scriptsize

| |  Emp Mean |  Emp Variance |  Theo Mean | Theo Variance |
|-|-:|-:|-:|-:|
| beta | 26606.41 | 18381480 | 26475.58 | 18584454 |
| gamm | 28141.08 | 222758930 | 28242.00 | 249582072 |
| logg | 28586.58 | 239426765 | 28842.05 | 280329757 |

(Empirical moments are calculated from the data,  
Theoretical are calculated with parameters estimates as per above)

## PML and quantiles

### PML

- Since  actual maxima can become infinite in insurance (in practical terms), we need to find a "reasonable" maximum to work with for capital and (mitigation) investment purposes (cost-benefit analysis)
- The question of "how bad" the losses can get cannot be  disentangled from frequency, as the longer the timeframe, the bigger the losses can be
- The answer is called a "probable maximum loss" (PML). It has a probability (or frequency) attached to it.


### Example 

```{r, fig.align='center', out.width="100%", echo=F}
knitr::include_graphics("table3.png")
```

"Brisbane River Strategic Floodplain Management Plan" (@QRA19)

"AEP" = "Annual Exceedance Proability"

### $t$-year events and PML


- Imagine you are focused on yearly outcomes, and that you want to estimate the outcome that you would ***\underline{expect}*** will happen ***\underline{on average}*** no more often than every 200 years (such as in solvency requirements in Australia)
- This is a "one-in-200-years" event, and is referred to as "$t$-year event" (where $t=200$ above), or outcome with a $t$-year "return period"
- This is what we will use to calculate a PML of corresponding probability $1/t$
- This is obviously impossible to estimate from data for large $t$, as it is simply not available - there is not enough information about the tail
- And this is even ignoring non stationarity (e.g., climate change)

---

- Note that that a $t$-year event can happen more often than every $t$ years! For instance:
  - The probability that a 200-year event will happen within the next 10 years is as high as `1-pbinom(0,10,1/200)` $= `r 1-pbinom(0,10,1/200)`$
  - The probability that it will happen more than twice in the next 50 years is `1-pbinom(1,20,1/200)` $=`r 1-pbinom(1,50,1/200)`$
  - These calculations become even more dramatic with slightly lower frequency events (1-in-100 or 1-in-50 year events)
  
  
### Example 

```{r, fig.align='center', out.width="100%", echo=F}
knitr::include_graphics("table1.png")
```

"Brisbane River Strategic Floodplain Management Plan" (@QRA19)



### Caution: what are you modelling?

- Although it is possible to get $t$-year quantiles from the GEV, it is not always obvious how to relate those to the above actuarial problem unless you are fitting portfolio level loss data
- It is not clear how to aggregate the maximum of a single loss to the maximum of a whole portfolio
- Because of this, the above results are generally used to model outcomes of the peril (e.g. flood level), not losses. Damages are subsequently mapped into aggregate losses
- The interpretation and fitting is easier in that context
  - for instance climate data, where you have daily maximum temperatures, and you may want to get a maximum with 200 day return period 
  - furthermore, the R package allows for seasonality

### Quantiles

For $t$-year events we seek $u_t$ such that $H_{\gamma;\mu,\sigma}(u_t)=1-1/t$, that is,
$$u_t = H_{\gamma;\mu,\sigma}^{-1}\left(1-1/t\right).$$
We have
$$
u_t= \begin{cases}
\mu+\frac{\sigma}{\gamma}\left[ \left(-\frac{1}{\log (1-1/t)}\right)^\gamma -1 \right]\quad & \text{ for }\gamma \neq 0. \\\\
\mu+\sigma \log  \left(-\frac{1}{\log (1-1/t)}\right) & \text{ for }\gamma = 0.
\end{cases}
$$

---

Close examination of those formulas tells us that the curvature of $u_t$  as a function of  $\log  \left(-1/\log (1-1/t)\right) \equiv y$ depends on the distribution:

- $\gamma<0$ (Weibull): concave with asymptotic upper limit as $t\rightarrow \infty$ at its upper bound $\mu-\sigma/\gamma$:
$$\mu +\frac{\sigma}{\gamma}\left(e^{\gamma y}-1\right) \longrightarrow_{y\rightarrow \infty} \mu-\sigma/\gamma$$
since $\gamma<0$.
- $\gamma=0$ (Gumbel): linear
- $\gamma>0$ (Fr\'echet)  convex with no finite upper bound



## Case study - quantiles

First, note that empirical $(1-1/t)$-th quantiles in the data ($X_{n(1/t),n}$) are
``` {r casequantiles}
ret.per <- c(10,50,100,200,500)
Xnn <- cbind(beta=sort(claims$beta,decreasing=TRUE),
             gamm=sort(claims$gamm,decreasing=TRUE),
             logg=sort(claims$logg,decreasing=TRUE))
empquant <- data.frame(Xnn[length(claims$beta)*1/ret.per,])
rownames(empquant) <- ret.per
empquant
```
These are **not** to be compared with the quantiles of the GEV.  
The GEV models the distribution of **maxima**.

---

Empirical quantiles in the blocks:
```{r casequantiles2}
Xnn2 <- cbind(betablock=sort(blockmax$betablock,decreasing=TRUE),
             gammblock=sort(blockmax$gammblock,decreasing=TRUE),
             loggblock=sort(blockmax$loggblock,decreasing=TRUE))
empquantblock <- data.frame(Xnn2[length(blockmax$betablock)*1/ret.per,])
rownames(empquantblock) <- ret.per
empquantblock
```

---

Theoretical quantiles in the blocks (maxima):
```{r casequantiles3}
parm <-cbind(as.double(fit.beta$results$par[1:3]),
             as.double(fit.gamm$results$par[1:3]),
             as.double(fit.logg$results$par[1:3]))
ut.theo <- data.frame(cbind(beta=parm[1,1]+parm[2,1]/parm[3,1]*((-1/(log(1-1/ret.per)))^parm[3,1]-1),
                 gamm=parm[1,2]+parm[2,2]/parm[3,2]*((-1/(log(1-1/ret.per)))^parm[3,2]-1),
                 logg=parm[1,3]+parm[2,3]/parm[3,3]*((-1/(log(1-1/ret.per)))^parm[3,3]-1)))
rownames(ut.theo) <- ret.per
ut.theo
```


---

Now this can be done with `extRemes` (with confidence intervals!)

```{r casequantiles4,output.lines=5:10}
return.level(fit.beta,return.period=ret.per,do.ci=TRUE)
return.level(fit.gamm,return.period=ret.per,do.ci=TRUE)
```

---

```{r casequantiles5,output.lines=5:10}
return.level(fit.logg,return.period=ret.per,do.ci=TRUE)
```


Theoretical figures are close to the empirical ones (as they should!), but obviously the theoretical versions are smoothed and will be less likely to underestimate extreme events, especially in high return level figures (which are more prone to sampling issues in the empirical data).



# Generalised Pareto distribution

## Asymptotic properties of the tails

### Main result: Asymptotic properties of the tails

- Let us now consider a distribution $F$ of losses $Y$. We are interested in the asymptotic behaviour of the tail so that we can approximate it due to lack of data [see also Chapter 4 of @Wut20; $S_{sc}$ vs $S_{lc}$]
- If we were pricing an excess of loss (or stop loss on a portfolio), the *attachment point* or *retention level* or *limit* could be expressed as a $t$-year event or quantile 
$$ u_t =F^{-1}\left( 1-\frac{1}{t}\right).$$
- The tail is the distribution beyond this point. We characterise the tail with the help of the distribution function of the excess over threshold $u$
$$F_{u}(x) = \Pr[Y-u\le x|Y>u].$$
- The **main result is that, *for high $u$*, $F_{u}(x)$ can be *approximated* by a Generalised Pareto**. (technically, as $u$ increases to the maximum possible value of $x$ then the absolute difference  
between both distributions is asymptotically zero)


## The GP distribution

### Generalised Pareto distribution

- The Generalised Pareto distribution function is given by
$$G_{\gamma,\sigma}(x) = \left\{ \begin{array}{cc} \displaystyle
1-\left( 1+ \gamma \frac{x}{\sigma}\right)_+^{-\frac{1}{\gamma}} & \gamma \neq 0 \\ \\ \displaystyle
1-\exp \left( -\frac{x}{\sigma}\right) & \gamma = 0
\end{array}\right.$$
for a scale parameter $\sigma>0$.
- We distinguish again three cases:
  - $\gamma<0$ (upper bound, also referred to as "Pareto Type II"): light tail, $X \in (0,\sigma/|\gamma|)$
  - $\gamma=0$ (exponential): base case $X \in \mathbb{R}$
  - $\gamma>0$ (Pareto): heavy tail, $X \in \mathbb{R}^+$ 

<!-- - If $G_{\gamma,\sigma}(x)$ is the limiting df of standardised maxima---that is, a GEV$(\mu,\sigma,\gamma)$, then the GP has the same \\ shape parameter $\gamma$ and modified scale parameter $\sigma -\gamma \mu$. -->

### GP df


```{r fig.align='center',out.width="75%", echo=F}
include_graphics(sprintf("%s/GPdf.png", graphics_path), error = F)
```

### GP pdf

```{r fig.align='center',out.width="75%", echo=F}
include_graphics(sprintf("%s/GPpdf.png", graphics_path), error = F)
```

- note $\xi \equiv \gamma$ [``(both graphs are from Wikipedia)``](https://en.wikipedia.org/wiki/Generalized_Pareto_distribution) 



### Moments

The first central moments of $X$, the excess of $u$, are

$$\begin{array}{rcl}
E[X] &=& \frac{\sigma}{1-\gamma}, \quad \gamma <1, \\
Var(X) &=& \frac{\sigma^2}{(1-\gamma)^2(1-2\gamma)}, \quad \gamma <1/2.
\end{array}$$

Note:

- The first four $k$ moments exist only for $\gamma<1/k$.
- $E[X]$ is the stop loss premium (since $X$ is the excess over threshold $u$)


## Estimation

### Estimation: choice of threshold $u$

We seek to estimate the tail (remember Tutorial Exercise `NLI11`)
$$\overline{F}(u+x)=\overline{F}(u)\overline{F}_u(x)$$
for a fixed large value $u$ and all $x\ge 0$. To do so, we need to estimate:

- Probability of exceeding $u$
$$ \widehat{\overline{F}(u)} = \frac{n_u}{n}$$
which will be more accurate for $u$ not too large.
- Then for given value of $u$
$$\widehat{\overline{F}_u(x)}=G_{\hat{\gamma},\hat{\sigma}}(x)$$
where $\hat{\gamma}(u)$ and $\hat{\sigma}(u)$ are estimated from the data in the tail above $u$ (via MLE). This approximation will work well only for large $u$  
(as the result is an asymptotic result for $u\rightarrow \infty$).


### Estimation: choice of threshold $u$

The choice of $u$ presents conflicting requirements for $u$:

- larger $u$ will lead to a better approximation from a distributional perspective
- but larger $u$ reduces the amount of data to estimate $\overline{F}(u)$ , $\gamma$ and $\sigma$

The choice is often also based on the following further considerations:

- mean-excess plot: Pareto should be linear (slope same sign as $\gamma$)
- empirical df on a doubly logarithmic scale: Pareto should be linear
- Hill plot (@Wut20, p. 76): stability of Pareto parameter
- stability of $\hat{\gamma}$ for different choices of $u$ (note this requires a full fit for each value of $u$ but the `extRemes` package does that automatically)
- goodness of fit for different choices of $u$

Unfortunately there is no "optimal" (or "automatic") procedure for choosing $u$.



## Case study

### Case study: workers compensation medical costs

- Example of real actuarial data from @AvCaWo11
- Data were provided by the SUVA (Swiss workers compensation insurer)
- Random sample of 5\% of accident claims in construction sector with accident year 1999 (developped as of 2003)
- Claims are medical costs
- In @AvCaWo11 we model those claims with a Gumbel distribution, which suggests heavy tails, which is confirmed by the following graphs. This, however, wasn't the main objective of the paper, but was good enough for our purposes.

```{r SUVAload,output.lines=1:5}
SUVA <-read_excel("SUVA.xls")
as_tibble(SUVA)
```

### Data exploration

```{r SUVA-CHF,fig.height=6}
plot(SUVA$medcosts,ylab = "Medical costs (CHF)", cex = 1.25,
     cex.lab = 1.25, col = "darkblue", bg = "lightblue", pch = 21)
```

---

```{r SUVA-logCHF,fig.height=7}
plot(log(SUVA$medcosts),ylab = "Medical costs (log CHF)", cex = 1.25,
     cex.lab = 1.25, col = "darkblue", bg = "lightblue", pch = 21)
```

---

```{r SUVA-logCHF-QQ,fig.height=7}
extRemes::qqnorm(log(SUVA$medcosts), cex.lab = 1.25)
```

### Size of tail

```{r SUVA-propclaims,fig.show="hide"}
numbexc <- c()
for (i in 250:20000) {
 numbexc <- c(numbexc,length(SUVA$medcosts[SUVA$medcosts>i]))
}

par(mfrow=c(1,2))
plot(250:20000,numbexc,xlab="Threshold ($)",ylab="Number of claims exceeding the threshold",col = "darkblue", bg = "lightblue", pch = 21)
plot(250:20000,numbexc/length(SUVA$medcosts),xlab="Threshold ($)",ylab="Proportion of claims above threshold",col = "darkblue", bg = "lightblue", pch = 21)

```

---

```{r SUVA-propclaims,echo=FALSE,fig.height=7.5}
```


### Mean-excess plot

```{r SUVA-threshold2,fig.height=6}
extRemes::mrlplot(SUVA$medcosts, xlim = c(250,20000))
```


### Empirical $\overline{F}$ on log log scale

```{r SUVA-loglog,fig.height=6,warning=FALSE}
evir::emplot(SUVA$medcosts,alog="xy",labels=TRUE)
```

### Hill plot

```{r SUVA-Hill,fig.height=5.5}
evir::hill(SUVA$medcosts)
```

### Stability of $\hat{\gamma}$

```{r SUVA-threshold1,fig.height=6,warning=FALSE,results="hide"}
extRemes::threshrange.plot(SUVA$medcosts, r = c(250,20000), nint = 80)
```



### Estimation results for $u=500$

```{r SUVA-fit-500-1,fig.show="hide",output.lines=10:25,collapse=TRUE}
fit.SUVA.500 <- fevd(SUVA$medcosts,threshold=500,type="GP",time.units = "1/year")
fit.SUVA.500
```

---


```{r SUVA-fit-500-2,fig.show="hide"}
numbexc[501-250] # n_u noting numbexc[1] = n_250
numbexc[501-250]/length(SUVA$medcosts)
plot(fit.SUVA.500)
```

---

```{r SUVA-fit-500-2,echo=FALSE,results="hide",fig.height=7}
```


### Estimation results for $u=5000$

```{r SUVA-fit-5000-1,fig.show="hide",output.lines=10:25,collapse=TRUE}
fit.SUVA.5000 <- fevd(SUVA$medcosts,threshold=5000,type="GP",time.units = "1/year")
fit.SUVA.5000
```

---


```{r SUVA-fit-5000-2,fig.show="hide"}
numbexc[5001-250] # n_u
numbexc[5001-250]/length(SUVA$medcosts)
plot(fit.SUVA.5000)
```

---

```{r SUVA-fit-5000-2,echo=FALSE,results="hide",fig.height=7}
```


### Estimation results for $u=10000$

```{r SUVA-fit-10000-1,fig.show="hide",output.lines=10:25,collapse=TRUE}
fit.SUVA.10000 <- fevd(SUVA$medcosts,threshold=10000,type="GP",time.units = "1/year")
fit.SUVA.10000
```

---


```{r SUVA-fit-10000-2,fig.show="hide"}
numbexc[10001-250] # n_u
numbexc[10001-250]/length(SUVA$medcosts)
plot(fit.SUVA.10000)
```

---

```{r SUVA-fit-10000-2,echo=FALSE,results="hide",fig.height=7}
```



### Estimation results for $u=12000$

```{r SUVA-fit-12000-1,fig.show="hide",output.lines=10:25,collapse=TRUE}
fit.SUVA.12000 <- fevd(SUVA$medcosts,threshold=12000,type="GP",time.units = "1/year")
fit.SUVA.12000
```

---


```{r SUVA-fit-12000-2,fig.show="hide"}
numbexc[12001-250] # n_u
numbexc[12001-250]/length(SUVA$medcosts)
plot(fit.SUVA.12000)
```

---

```{r SUVA-fit-12000-2,echo=FALSE,results="hide",fig.height=7}
```




# Measures of tail weight


- Existence of moments (e.g. gamma vs Pareto)
- Limiting density ratios: relative value of density functions at the far end of the upper tail of two distributions
- Hazard rates $\lambda(x)=f(x)/(1-F(x))$: constant for exponential, decreasing for heavy tails
- Log-log plot: linear decrease for heavy tails
- Mean excess function: linear increase for heavy tails


### Limiting density ratios: gamma example

```{r EVT-gammatoexp,fig.show="hide"}
plot(1:10000/1000,
     dgamma(1:10000/1000,shape=0.75,rate=.75)/dexp(1:10000/1000,1),
     xlab="x",main="ratio of densities gamma(alpha,beta) to exponential(1)",ylab="",
     type="l",col="blue",ylim=c(0,3.5),xlim=c(0,6),lwd = 2,cex=1.5)
text(5.5,2,"alpha=beta=0.75",col="blue",cex=1.5)
lines(1:10000/1000,
     dgamma(1:10000/1000,shape=1,rate=1)/dexp(1:10000/1000,1),
     type="l",col="black",lwd = 2)
text(5.5,1.2,"alpha=beta=1",col="black",cex=1.5)
lines(1:10000/1000,
      dgamma(1:10000/1000,shape=2,rate=2)/dexp(1:10000/1000,1),
      type="l",col="red",lwd = 2)
text(5.5,0.3,"alpha=beta=2",col="red",cex=1.5)
lines(1:10000/1000,
      dgamma(1:10000/1000,shape=10,rate=10)/dexp(1:10000/1000,1),
      type="l",col="green",lwd = 2)
text(2.8,0.2,"alpha=beta=10",col="green",cex=1.5)
lines(1:10000/1000,
      dgamma(1:10000/1000,shape=.5,rate=.5)/dexp(1:10000/1000,1),
      type="l",col="magenta",lwd = 2)
text(5.5,3.5,"alpha=beta=0.5",col="magenta",cex=1.5)
```

---

```{r EVT-gammatoexp,echo=FALSE,fig.height=8}
```


### Hazard rates: gamma example with fixed mean

```{r EVT-gammafailure,fig.show="hide"}
plot(1:10000/1000,
     dgamma(1:10000/1000,shape=.5,rate=.5)/(1-pgamma(1:10000/1000,shape=.5,rate=.5)),
     xlab="x",main="hazard rates for gamma(alpha,beta)",ylab="",
     type="l",col="blue",ylim=c(0,4),xlim=c(0,5),lwd = 2,cex=1.5)
text(4.5,.75,"alpha=beta=0.5",col="blue",cex=1.5)
lines(1:10000/1000,
      dgamma(1:10000/1000,shape=1,rate=1)/(1-pgamma(1:10000/1000,shape=1,rate=1)),
      type="l",col="black",lwd = 2)
text(4.5,1.2,"alpha=beta=1",col="black",cex=1.5)
lines(1:10000/1000,
      dgamma(1:10000/1000,shape=2,rate=2)/(1-pgamma(1:10000/1000,shape=2,rate=2)),
      type="l",col="red",lwd = 2)
text(4.5,2,"alpha=beta=2",col="red",cex=1.5)
lines(1:10000/1000,
      dgamma(1:10000/1000,shape=4,rate=4)/(1-pgamma(1:10000/1000,shape=4,rate=4)),
      type="l",col="green",lwd = 2)
text(4.5,3.6,"alpha=beta=4",col="green",cex=1.5)
lines(1:10000/1000,
      dgamma(1:10000/1000,shape=.1,rate=.1)/(1-pgamma(1:10000/1000,shape=.1,rate=.1)),
      type="l",col="magenta",lwd = 2)
text(4.5,.35,"alpha=beta=0.1",col="magenta",cex=1.5)
```

---

```{r EVT-gammafailure,echo=FALSE,fig.height=8}
```

<!-- ### Hazard rates: gamma example with fixed mean -->

<!-- ```{r EVT-gammafailure2,fig.show="hide"} -->
<!-- plot(1:10000/1000, -->
<!--      dgamma(1:10000/1000,shape=.75,rate=1)/(1-pgamma(1:10000/1000,shape=.75,rate=1)), -->
<!--      xlab="x",main="failure rates for gamma(alpha,beta=1)",ylab="", -->
<!--      type="l",col="blue",ylim=c(0,2.5),xlim=c(0,5),lwd = 2,cex=1.5) -->
<!-- text(2.5,1.15,"alpha=0.75",col="blue",cex=1.5) -->
<!-- lines(1:10000/1000, -->
<!--       dgamma(1:10000/1000,shape=1,rate=1)/(1-pgamma(1:10000/1000,shape=1,rate=1)), -->
<!--       type="l",col="black",lwd = 2,cex=1.5) -->
<!-- text(2.5,.9,"alpha=1",col="black",cex=1.5) -->
<!-- lines(1:10000/1000, -->
<!--       dgamma(1:10000/1000,shape=2,rate=1)/(1-pgamma(1:10000/1000,shape=2,rate=1)), -->
<!--       type="l",col="red",lwd = 2,cex=1.5) -->
<!-- text(2.5,.6,"alpha=2",col="red",cex=1.5) -->
<!-- lines(1:10000/1000, -->
<!--       dgamma(1:10000/1000,shape=4,rate=1)/(1-pgamma(1:10000/1000,shape=4,rate=1)), -->
<!--       type="l",col="green",lwd = 2) -->
<!-- text(2.5,0.15,"alpha=4",col="green",cex=1.5) -->
<!-- lines(1:10000/1000, -->
<!--       dgamma(1:10000/1000,shape=.01,rate=1)/(1-pgamma(1:10000/1000,shape=.01,rate=1)), -->
<!--       type="l",col="magenta",lwd = 2) -->
<!-- text(2.5,1.45,"alpha=0.01",col="magenta",cex=1.5) -->
<!-- ``` -->

<!-- --- -->

<!-- ```{r EVT-gammafailure2,echo=FALSE,fig.height=8} -->
<!-- ``` -->



# References 
